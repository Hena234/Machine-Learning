{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def func():\n",
    "  data = np.loadtxt(\"mnist_train.csv\", delimiter=\",\", skiprows=1, dtype=int)\n",
    "  y, x = np.hsplit(data,[1])\n",
    "  return y,x/255\n",
    "\n",
    "y,x= func()\n",
    "y.flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10) (60000, 10) (784, 60000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.9604248 , 0.94238907, 0.23483756, ..., 0.403641  , 0.02026011,\n",
       "         0.14549154],\n",
       "        [0.43389513, 0.63420531, 0.54262156, ..., 0.05705563, 0.95786844,\n",
       "         0.44618074],\n",
       "        [0.26335497, 0.98653989, 0.27422922, ..., 0.68883013, 0.44493636,\n",
       "         0.30447472],\n",
       "        ...,\n",
       "        [0.90120846, 0.78754029, 0.42351383, ..., 0.58226873, 0.8456259 ,\n",
       "         0.83301621],\n",
       "        [0.8307575 , 0.73947022, 0.15837532, ..., 0.89726699, 0.99982332,\n",
       "         0.80035027],\n",
       "        [0.61338029, 0.46742161, 0.59543246, ..., 0.99241931, 0.72759747,\n",
       "         0.04054996]]),\n",
       " array([[0.68158607, 0.02582935, 0.18991323, ..., 0.10607882, 0.92364867,\n",
       "         0.34503567],\n",
       "        [0.68158607, 0.02582935, 0.18991323, ..., 0.10607882, 0.92364867,\n",
       "         0.34503567],\n",
       "        [0.68158607, 0.02582935, 0.18991323, ..., 0.10607882, 0.92364867,\n",
       "         0.34503567],\n",
       "        ...,\n",
       "        [0.68158607, 0.02582935, 0.18991323, ..., 0.10607882, 0.92364867,\n",
       "         0.34503567],\n",
       "        [0.68158607, 0.02582935, 0.18991323, ..., 0.10607882, 0.92364867,\n",
       "         0.34503567],\n",
       "        [0.68158607, 0.02582935, 0.18991323, ..., 0.10607882, 0.92364867,\n",
       "         0.34503567]]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class neural_net:\n",
    "  \n",
    "  def neural(self,x,y,ler):\n",
    "\n",
    "    w=np.random.rand(784,10)\n",
    "    b=np.random.rand(1,10)\n",
    "    z= np.add(np.dot(x,w),b)\n",
    "    \n",
    "    \n",
    "    y_pred=np.exp(z)/np.sum(np.exp(z))\n",
    "    y_one_hot_vec=np.eye(10)[y.flatten()]\n",
    "\n",
    "    dz = y_pred-y_one_hot_vec\n",
    "    cost = -np.mean(y_one_hot_vec * np.log(y_pred + 1e-8))\n",
    "    \n",
    "    \n",
    "\n",
    "    dw = np.dot(x.T,dz)/len(y)\n",
    "    print(dw.shape,dz.shape,x.T.shape)\n",
    "    db = 1/len(y) * np.sum(dz, axis=1, keepdims=True)\n",
    "    w = w-ler*dw\n",
    "    b = b-ler*db\n",
    "\n",
    "    return w,b\n",
    "\n",
    "ner=neural_net()\n",
    "ner.neural(x,y,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "10\n",
      "(60000, 10) (60000, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'weights_hidden': array([[ 0.05482224, -0.00037451, -0.03916996, ..., -0.1026107 ,\n",
       "         -0.18315194,  0.16806182],\n",
       "        [-0.02144295, -0.07869935, -0.11124494, ...,  0.03705442,\n",
       "          0.09872   ,  0.06224514],\n",
       "        [ 0.07307658, -0.00406123, -0.09922423, ...,  0.03307397,\n",
       "         -0.03635271,  0.11832137],\n",
       "        ...,\n",
       "        [-0.12361992,  0.01776383,  0.07368684, ...,  0.00710718,\n",
       "         -0.04213552, -0.1379978 ],\n",
       "        [-0.12326638,  0.07832614, -0.01110684, ..., -0.04154094,\n",
       "         -0.01107898,  0.06966505],\n",
       "        [-0.11215994, -0.00782288, -0.02474881, ..., -0.01877054,\n",
       "         -0.05782368, -0.01694113]]),\n",
       " 'biases_hidden': array([[-0.02218827, -0.02218827, -0.02218827, ..., -0.02218827,\n",
       "         -0.02218827, -0.02218827],\n",
       "        [-0.02365385, -0.02365385, -0.02365385, ..., -0.02365385,\n",
       "         -0.02365385, -0.02365385],\n",
       "        [-0.02191024, -0.02191024, -0.02191024, ..., -0.02191024,\n",
       "         -0.02191024, -0.02191024],\n",
       "        ...,\n",
       "        [-0.02218705, -0.02218705, -0.02218705, ..., -0.02218705,\n",
       "         -0.02218705, -0.02218705],\n",
       "        [-0.02216211, -0.02216211, -0.02216211, ..., -0.02216211,\n",
       "         -0.02216211, -0.02216211],\n",
       "        [-0.02207283, -0.02207283, -0.02207283, ..., -0.02207283,\n",
       "         -0.02207283, -0.02207283]]),\n",
       " 'weights_output': array([[-0.98118529, -0.94507193, -1.15104703, -0.89288202, -0.8170364 ,\n",
       "         -0.93877307, -1.03525769, -0.9652334 , -1.06868469, -0.90545775,\n",
       "         -0.89601317, -1.08447391, -0.65208864, -0.97425542, -0.83232536],\n",
       "        [-1.40411209, -1.1774103 , -1.38033543, -1.03032431, -1.18897045,\n",
       "         -1.37438977, -1.1555169 , -1.26329439, -1.26391825, -1.32994279,\n",
       "         -1.17482334, -1.17798341, -1.32409915, -1.31469607, -1.26774308],\n",
       "        [-1.04276957, -1.15705034, -1.04639516, -1.05412221, -1.05572313,\n",
       "         -1.07582132, -0.96650376, -1.09654009, -1.15455669, -1.06422577,\n",
       "         -0.97026134, -0.92058009, -1.10253933, -0.90572013, -0.97991355],\n",
       "        [-1.1668575 , -1.0898897 , -1.18428585, -1.10956442, -1.22507095,\n",
       "         -1.35819388, -1.17427493, -0.91483853, -1.17238773, -1.20959761,\n",
       "         -1.22702427, -1.25675335, -1.02564261, -1.07195897, -1.11757117],\n",
       "        [-1.34665549, -1.38389073, -1.16985758, -1.30424506, -1.19906671,\n",
       "         -1.14999968, -1.25699522, -1.21226446, -1.16987078, -1.25734247,\n",
       "         -1.1276436 , -1.10749203, -1.13963945, -1.42340862, -1.26845421],\n",
       "        [-1.18602231, -1.13968354, -1.15207844, -1.01744926, -1.23683767,\n",
       "         -1.24665743, -1.15382009, -1.16500231, -1.17257017, -1.39805912,\n",
       "         -1.11906414, -1.27560653, -1.22190515, -1.21739547, -1.17633222],\n",
       "        [-1.28280384, -1.13868443, -1.10090528, -1.27662975, -1.08658097,\n",
       "         -1.14473601, -1.34713791, -1.30079948, -1.08263952, -1.20754223,\n",
       "         -1.1049863 , -1.17294733, -1.10109744, -1.3212851 , -1.20671234],\n",
       "        [-0.82789257, -0.96110476, -1.00265386, -0.84386094, -0.9244603 ,\n",
       "         -0.86856792, -0.73316534, -0.93462887, -0.97000045, -0.95954314,\n",
       "         -0.97754127, -0.90425328, -0.84517215, -0.88900987, -0.93747486],\n",
       "        [-1.18400614, -1.3200449 , -1.30579925, -1.21869379, -1.12867753,\n",
       "         -1.23838184, -1.29221874, -1.31759957, -1.17645223, -1.05808829,\n",
       "         -1.2613991 , -1.20312805, -1.06386839, -1.17367031, -1.16980066],\n",
       "        [-0.829675  , -0.85738352, -1.05404735, -1.00769833, -0.93996082,\n",
       "         -0.8729224 , -0.93744125, -1.18721034, -1.00791111, -0.85093309,\n",
       "         -1.06253923, -0.80645871, -0.89780881, -0.91916181, -0.91686987]]),\n",
       " 'biases_output': array([[-0.00037172, -0.00037172, -0.00037172, ..., -0.00037172,\n",
       "         -0.00037172, -0.00037172],\n",
       "        [-0.00037175, -0.00037175, -0.00037175, ..., -0.00037175,\n",
       "         -0.00037175, -0.00037175],\n",
       "        [-0.00037172, -0.00037172, -0.00037172, ..., -0.00037172,\n",
       "         -0.00037172, -0.00037172],\n",
       "        ...,\n",
       "        [-0.0003717 , -0.0003717 , -0.0003717 , ..., -0.0003717 ,\n",
       "         -0.0003717 , -0.0003717 ],\n",
       "        [-0.00037171, -0.00037171, -0.00037171, ..., -0.00037171,\n",
       "         -0.00037171, -0.00037171],\n",
       "        [-0.00037174, -0.00037174, -0.00037174, ..., -0.00037174,\n",
       "         -0.00037174, -0.00037174]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class network:\n",
    "    def __init__(self, LEARNING_RATE):\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "\n",
    "    def one_hot(self,y):\n",
    "\n",
    "        y_one_hot=np.eye(10)[y.flatten()]\n",
    "        return y_one_hot\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        # Sigmoid activation function (returns a number between 0 and 1)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def define_neurons(self,X, Y,n):\n",
    "        Y = self.one_hot(Y) # Convert the labels into \n",
    "        input_layer_neurons = X.shape[0] # size of input layer\n",
    "        # print(X.shape[0])\n",
    "        hidden_layer_neurons = n # hidden layer of size n\n",
    "        ouput_layer_neurons = 10 # size of output layer\n",
    "        return (input_layer_neurons, hidden_layer_neurons, ouput_layer_neurons)\n",
    "        \n",
    "\n",
    "    def initialize_parameters(self,X, hidden_layer_neurons, ouput_layer_neurons):\n",
    "        print(hidden_layer_neurons)\n",
    "        print(ouput_layer_neurons)\n",
    "        # Initialize the weights and biases for the nodes in the hidden layer\n",
    "        weights_hidden = np.random.randn(X.shape[1],hidden_layer_neurons) * 0.1 # Random weights\n",
    "        biases_hidden = np.zeros(( 1,hidden_layer_neurons)) # Zeroed biases\n",
    "\n",
    "        # Initialize the weights and biases for the nodes in the output layer\n",
    "        weights_output = np.random.randn(hidden_layer_neurons,ouput_layer_neurons) * 0.1 # Random weights\n",
    "        biases_output = np.zeros(( 1,ouput_layer_neurons)) # Zeroed biases\n",
    "\n",
    "        self.parameters = {\"weights_hidden\": weights_hidden,\n",
    "                    \"biases_hidden\": biases_hidden,\n",
    "                    \"weights_output\": weights_output,\n",
    "                    \"biases_output\": biases_output}\n",
    "        \n",
    "        return self.parameters\n",
    "        \n",
    "    def forward_propagation(self,X, parameters):\n",
    "        # Extract weights and biases\n",
    "        weights_hidden, biases_hidden, weights_output, biases_output = parameters['weights_hidden'], parameters['biases_hidden'], parameters['weights_output'], parameters['biases_output']\n",
    "    \n",
    "        # Raw output of hidden layer \n",
    "        unactivated_hidden = np.add(np.dot( X,weights_hidden),biases_hidden)\n",
    "\n",
    "        # sigmoid activated output of hidden layer\n",
    "        activated_hidden = self.sigmoid(unactivated_hidden)\n",
    "        # print(activated_hidden.shape)\n",
    "\n",
    "        # Raw output of output layer\n",
    "        # print(activated_hidden.shape,weights_output.shape)\n",
    "        unactivated_output = np.add(np.dot(activated_hidden,weights_output ),biases_output)\n",
    "        # print(unactivated_output.shape)\n",
    "\n",
    "        # Sigmoid activated output of output layer\n",
    "        activated_output = self.sigmoid(unactivated_output)\n",
    "\n",
    "        outputs = {\"unactivated_hidden\": unactivated_hidden,\n",
    "                \"activated_hidden\": activated_hidden,\n",
    "                \"unactivated_output\": unactivated_output,\n",
    "                \"activated_output\": activated_output}\n",
    "        \n",
    "        return activated_output, outputs\n",
    "\n",
    "    def log_cost(self,activated_output, Y, parameters):\n",
    "        m = Y.shape[0] \n",
    "\n",
    "        Y = self.one_hot(Y)\n",
    "        # print(Y.shape,activated_output.shape)\n",
    "\n",
    "        # Calculate Costs\n",
    "        cost =-np.mean(Y* np.log(activated_output+ 1e-8))\n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self,parameters, outputs, X, Y):\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        Y = self.one_hot(Y)\n",
    "\n",
    "        # Extract the weights, biases and neuron outputs\n",
    "\n",
    "        weights_hidden = parameters['weights_hidden']\n",
    "        weights_output = parameters['weights_output']\n",
    "        activated_hidden = outputs['activated_hidden']\n",
    "        activated_output = outputs['activated_output']\n",
    "\n",
    "\n",
    "        # Backward propagation gradients\n",
    "        dz2 = activated_output - Y\n",
    "        print(dz2.shape,activated_hidden.shape)\n",
    "\n",
    "        dweights_output = np.dot(dz2.T,activated_hidden )/m\n",
    "        dbiases_output = 1/m * np.sum(dz2, axis=1, keepdims=True)\n",
    "\n",
    "        dz1 = np.dot(dz2,dweights_output) * (1-activated_hidden)*activated_hidden\n",
    "        dweights_hidden = np.dot( X.T,dz1)/m\n",
    "        dbiases_hidden = 1/m * np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        gradients = {\"dweights_hidden\": dweights_hidden, \"dbiases_hidden\": dbiases_hidden, \"dweights_output\": dweights_output,\"dbiases_output\": dbiases_output}\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "    def gradient_descent(self,parameters, gradients):\n",
    "        # Extract weights and biases\n",
    "        weights_hidden = parameters['weights_hidden']\n",
    "        biases_hidden = parameters['biases_hidden']\n",
    "        weights_output = parameters['weights_output']\n",
    "        biases_output = parameters['biases_output']\n",
    "    \n",
    "        # Extract gradients to apply \n",
    "        dweights_hidden = gradients['dweights_hidden']\n",
    "        dbiases_hidden = gradients['dbiases_hidden']\n",
    "        dweights_output = gradients['dweights_output']\n",
    "        dbiases_output = gradients['dbiases_output']\n",
    "\n",
    "        # Apply gradients to weights and biases\n",
    "        weights_hidden = weights_hidden - self.LEARNING_RATE * dweights_hidden\n",
    "        biases_hidden = biases_hidden - self.LEARNING_RATE * dbiases_hidden\n",
    "        weights_output = weights_output.T - LEARNING_RATE * dweights_output\n",
    "        biases_output = biases_output - LEARNING_RATE * dbiases_output\n",
    "        \n",
    "        # New weights and biases\n",
    "        parameters = {\"weights_hidden\": weights_hidden, \"biases_hidden\": biases_hidden,\"weights_output\": weights_output,\"biases_output\": biases_output}\n",
    "        \n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def model(self,X, Y,n):\n",
    "\n",
    "        # Define the # of neurons for each layer based on input and output configurations\n",
    "        (input_layer_neurons, hidden_layer_neurons, ouput_layer_neurons) = self.define_neurons(X, Y,n)\n",
    "\n",
    "        # Initialize weights and biases for the amount of neurons specified previously \n",
    "        parameters = self.initialize_parameters(X, hidden_layer_neurons, ouput_layer_neurons)\n",
    "        \n",
    "        # Get output of the model with inputs and current parameters\n",
    "        activated_output, outputs = self.forward_propagation(X, parameters)\n",
    "\n",
    "        # Calculate the cost \n",
    "        cost = self.log_cost(activated_output, Y, parameters)\n",
    "\n",
    "        # Calculate gradients \n",
    "        gradients = self.backward_propagation(parameters, outputs, X, Y)\n",
    "\n",
    "        # Apply gradients and calculate parameters\n",
    "        parameters = self.gradient_descent(parameters, gradients)\n",
    "\n",
    "        return parameters\n",
    "\n",
    "LEARNING_RATE = 0.07\n",
    "\n",
    "object=network(LEARNING_RATE)\n",
    "object.model(x/255,y,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.04167578, -0.00562668, -0.21361961, ..., -0.06168445,\n",
      "         0.03213358, -0.09464469],\n",
      "       [-0.05301394, -0.1259207 ,  0.16775441, ..., -0.03284246,\n",
      "        -0.05623108,  0.01179136],\n",
      "       [ 0.07386378, -0.15872956,  0.01532001, ..., -0.08428557,\n",
      "         0.10040469,  0.00545832]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.06650944, -0.19626047,  0.2112715 ],\n",
      "       [-0.28074571, -0.13967752,  0.02641189],\n",
      "       [ 0.10925169,  0.06646016,  0.08565535],\n",
      "       [-0.11058228,  0.03715795,  0.13440124],\n",
      "       [-0.16421272, -0.1153127 ,  0.02013163],\n",
      "       [ 0.13985659,  0.07228733, -0.10717236],\n",
      "       [-0.05673344, -0.03663499, -0.15460347]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[ 0.20406947, -0.04960206, -0.06131668, -0.17449682,  0.01840741,\n",
      "        -0.00795452,  0.12406296],\n",
      "       [ 0.08625738, -0.01239074,  0.05854764,  0.19336815, -0.07322525,\n",
      "        -0.039205  ,  0.07512152],\n",
      "       [-0.05947453,  0.04753173, -0.1083593 ,  0.08228398,  0.07047718,\n",
      "        -0.05854343, -0.12699409],\n",
      "       [ 0.03255273,  0.00457369, -0.13662463,  0.10202692, -0.07310626,\n",
      "         0.1496712 ,  0.13433165],\n",
      "       [ 0.02566371,  0.0734615 , -0.14332651,  0.00178312,  0.05686418,\n",
      "        -0.1263975 , -0.14590294],\n",
      "       [ 0.15906599,  0.04026281,  0.14249133,  0.10019812, -0.28192685,\n",
      "        -0.11228612, -0.01523209],\n",
      "       [ 0.00556535,  0.01378749, -0.0675063 , -0.00885622, -0.10151087,\n",
      "         0.12861383, -0.09708002],\n",
      "       [-0.0577768 ,  0.08917285, -0.05625892,  0.01765442, -0.09055266,\n",
      "        -0.00368937,  0.04094553],\n",
      "       [-0.15298018, -0.16785625, -0.116733  ,  0.08260156,  0.05470732,\n",
      "         0.08330186,  0.14913897],\n",
      "       [-0.04016882, -0.07274709, -0.01175106,  0.0241847 ,  0.10988869,\n",
      "         0.01330499,  0.05696497]]), 'b3': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "class networks:\n",
    "    def one_hot(self,j):\n",
    "\n",
    "        n = j.shape[0]\n",
    "        oneh = np.zeros((10, n))\n",
    "        index = 0\n",
    "        for res in j:\n",
    "            oneh[res][index] = 1.0\n",
    "            index = index + 1\n",
    "        return oneh\n",
    "\n",
    "    def sigmoid(Z):\n",
    "        \n",
    "        H = 1/(1+np.exp(-Z))\n",
    "        sigmoid_memory = Z\n",
    "        \n",
    "        return H, sigmoid_memory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def softmax(Z):\n",
    "        # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "\n",
    "        Z_exp = np.exp(Z)\n",
    "\n",
    "        Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "        \n",
    "        H = Z_exp/Z_sum  #normalising step\n",
    "        softmax_memory = Z\n",
    "        \n",
    "        return H, softmax_memory\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_parameters(self,dimensions):\n",
    "\n",
    "        # dimensions is a list containing the number of neuron in each layer in the network\n",
    "        # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "        np.random.seed(2)\n",
    "        parameters = {}\n",
    "        L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "        for l in range(1, L): \n",
    "            parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "            parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "            \n",
    "            assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "            assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "            \n",
    "        return parameters\n",
    "\n",
    "\n",
    "    dimensions  = [784, 10,10,10]\n",
    "    # parameters = initialize_parameters(dimensions)\n",
    "\n",
    "    def layer_forward(self,H_prev, W, b, activation = 'sigmoid'):\n",
    "\n",
    "        # H_prev is of shape (size of previous layer, number of examples)\n",
    "        # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "        # b is bias vector of shape (size of the current layer, 1)\n",
    "    # H is the output of the activation function \n",
    "        \n",
    "        if activation == \"sigmoid\":\n",
    "            Z = np.dot(W, H_prev)+b\n",
    "            linear_memory = (H_prev, W, b)\n",
    "            H, activation_memory = self.sigmoid(Z) #\n",
    "    \n",
    "        elif activation == \"softmax\":\n",
    "            Z = np.dot(W, H_prev)+b#\n",
    "            linear_memory = (H_prev, W, b)\n",
    "            H, activation_memory = self.softmax(Z) #\n",
    "        \n",
    "            \n",
    "        assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "        memory = (linear_memory, activation_memory)\n",
    "\n",
    "        return H, memory\n",
    "\n",
    "    \n",
    "\n",
    "    def L_layer_forward(self,X, parameters):\n",
    "\n",
    "        # X is input data of shape (input size, number of examples\n",
    "        # HL is the last layer's post-activation value\n",
    "        \n",
    "\n",
    "        memories = []\n",
    "        H = X\n",
    "        L = len(parameters) // 2                  # number of layers in the neural network\n",
    "        \n",
    "        # Implement sigmoid layer (L-1) times as the Lth layer is the softmax layer\n",
    "        for l in range(1, L):\n",
    "            H_prev = H\n",
    "            W_l = parameters['W'+str(l)]\n",
    "            b_l = parameters['b'+str(l)]\n",
    "            H, memory = self.layer_forward(H_prev, W_l , b_l , activation = 'sigmoid')\n",
    "            memories.append(memory)\n",
    "        \n",
    "        # Implement the final softmax layer\n",
    "        # HL here is the final prediction P as specified in the lectures\n",
    "        W_L = parameters['W'+str(L)]\n",
    "        b_L = parameters['b'+str(L)]\n",
    "        HL, memory = self.layer_forward(H, W_L, b_L, activation = 'softmax') #\n",
    "        \n",
    "        memories.append(memory)\n",
    "\n",
    "        assert(HL.shape == (10, X.shape[1]))\n",
    "                \n",
    "        return HL, memories\n",
    "\n",
    "    def compute_loss(self,HL, Y):\n",
    "        \n",
    "        # loss is the cross-entropy loss\n",
    "\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        loss = -(1.0/m) * np.sum(Y*np.log(HL)) \n",
    "        loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "        assert(loss.shape == ())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid_backward(self,dH, sigmoid_memory):\n",
    "        \n",
    "        # Implement the backpropagation of a sigmoid function\n",
    "        # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "        # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "        \n",
    "        Z = sigmoid_memory\n",
    "        \n",
    "        H = 1/(1+np.exp(-Z))\n",
    "        dZ = dH * H * (1-H)\n",
    "        \n",
    "        assert (dZ.shape == Z.shape)\n",
    "        \n",
    "        return dZ\n",
    "\n",
    "\n",
    "\n",
    "    def layer_backward(self,dH, memory):\n",
    "        \n",
    "        # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "        \n",
    "\n",
    "        linear_memory, activation_memory = memory\n",
    "        dZ = self.sigmoid_backward(dH, activation_memory)#\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1.0/m) * np.matmul(dZ, H_prev.T)#, use (1./m) and not (1/m)\n",
    "        db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)#, use (1./m) and not (1/m)\n",
    "        dH_prev = np.matmul(np.transpose(W), dZ)#\n",
    "        \n",
    "        return dH_prev, dW, db\n",
    "\n",
    "    def L_layer_backward(self,HL, Y, memories):\n",
    "        \n",
    "        # Takes the predicted value HL and the true target value Y and the \n",
    "        # memories calculated by L_layer_forward as input\n",
    "        \n",
    "        # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "        gradients = {}\n",
    "        L = len(memories) # the number of layers\n",
    "        m = HL.shape[1]\n",
    "        # Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "        \n",
    "        # Perform the backprop for the last layer that is the softmax layer\n",
    "        current_memory = memories[-1]\n",
    "        linear_memory, activation_memory = current_memory\n",
    "        dZ = HL - Y\n",
    "        H_prev, W, b = linear_memory\n",
    "        # Use the expressions you have used in 'layer_backward'\n",
    "        gradients[\"dH\" + str(L-1)] = np.dot(np.transpose(W),dZ)#\n",
    "        gradients[\"dW\" + str(L)] = (1.0/m) * np.matmul(dZ, H_prev.T)\n",
    "        gradients[\"db\" + str(L)] = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "\n",
    "        # Perform the backpropagation l-1 times\n",
    "        for l in reversed(range(L-1)):\n",
    "            # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "            current_memory = memories[l]\n",
    "            \n",
    "            dH_prev_temp, dW_temp, db_temp = self.layer_backward(gradients[\"dH\"+ str(l+1)] ,current_memory)#\n",
    "            gradients[\"dH\" + str(l)] = dH_prev_temp#\n",
    "            gradients[\"dW\" + str(l + 1)] = dW_temp#\n",
    "            gradients[\"db\" + str(l + 1)] = db_temp#\n",
    "\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update_parameters(self,parameters, gradients, learning_rate):\n",
    "\n",
    "        # parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "        # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "        \n",
    "        # returns updated weights after applying the gradient descent update\n",
    "\n",
    "        \n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*gradients[\"dW\" + str(l + 1)] #\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*gradients[\"db\" + str(l + 1)]#\n",
    "\n",
    "            \n",
    "        return parameters\n",
    "    print(parameters)\n",
    "\n",
    "    dimensions = [784, 10, 10]\n",
    "\n",
    "    def L_layer_model(self,X, Y, dimensions, learning_rate):\n",
    "        \n",
    "        # returns updated parameters\n",
    "\n",
    "        # np.random.seed(2)\n",
    "        # losses = []                         # keep track of loss\n",
    "        \n",
    "        # Parameters initialization\n",
    "        parameters = self.initialize_parameters(self,dimensions)\n",
    "\n",
    "\n",
    "        # Forward propagation\n",
    "        HL, memories =  self.L_layer_forward(self,X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.compute_loss(self,HL, Y)\n",
    "    # Backward propagation\n",
    "        gradients = self.L_layer_backward(self,HL,Y, memories)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = self.update_parameters(self,parameters, gradients, learning_rate)\n",
    "                    \n",
    "        \n",
    "obj=networks()\n",
    "obj.L_layer_model(x.T,y.T,dimensions= [784,10,10,10],learning_rate=0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.04167578, -0.00562668, -0.21361961, ..., -0.06168445,\n",
      "         0.03213358, -0.09464469],\n",
      "       [-0.05301394, -0.1259207 ,  0.16775441, ..., -0.03284246,\n",
      "        -0.05623108,  0.01179136],\n",
      "       [ 0.07386378, -0.15872956,  0.01532001, ..., -0.08428557,\n",
      "         0.10040469,  0.00545832]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.06650944, -0.19626047,  0.2112715 ],\n",
      "       [-0.28074571, -0.13967752,  0.02641189],\n",
      "       [ 0.10925169,  0.06646016,  0.08565535],\n",
      "       [-0.11058228,  0.03715795,  0.13440124],\n",
      "       [-0.16421272, -0.1153127 ,  0.02013163],\n",
      "       [ 0.13985659,  0.07228733, -0.10717236],\n",
      "       [-0.05673344, -0.03663499, -0.15460347]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[ 0.20406947, -0.04960206, -0.06131668, -0.17449682,  0.01840741,\n",
      "        -0.00795452,  0.12406296],\n",
      "       [ 0.08625738, -0.01239074,  0.05854764,  0.19336815, -0.07322525,\n",
      "        -0.039205  ,  0.07512152],\n",
      "       [-0.05947453,  0.04753173, -0.1083593 ,  0.08228398,  0.07047718,\n",
      "        -0.05854343, -0.12699409],\n",
      "       [ 0.03255273,  0.00457369, -0.13662463,  0.10202692, -0.07310626,\n",
      "         0.1496712 ,  0.13433165],\n",
      "       [ 0.02566371,  0.0734615 , -0.14332651,  0.00178312,  0.05686418,\n",
      "        -0.1263975 , -0.14590294],\n",
      "       [ 0.15906599,  0.04026281,  0.14249133,  0.10019812, -0.28192685,\n",
      "        -0.11228612, -0.01523209],\n",
      "       [ 0.00556535,  0.01378749, -0.0675063 , -0.00885622, -0.10151087,\n",
      "         0.12861383, -0.09708002],\n",
      "       [-0.0577768 ,  0.08917285, -0.05625892,  0.01765442, -0.09055266,\n",
      "        -0.00368937,  0.04094553],\n",
      "       [-0.15298018, -0.16785625, -0.116733  ,  0.08260156,  0.05470732,\n",
      "         0.08330186,  0.14913897],\n",
      "       [-0.04016882, -0.07274709, -0.01175106,  0.0241847 ,  0.10988869,\n",
      "         0.01330499,  0.05696497]]), 'b3': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "def one_hot(j):\n",
    "   \n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    sigmoid_memory = Z\n",
    "    \n",
    "    return H, sigmoid_memory\n",
    "\n",
    "Z = np.arange(8).reshape(4,2)\n",
    "\n",
    "def relu(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "    # relu_memory is stored as it is used later on in backpropagation\n",
    "    \n",
    "    H = np.maximum(0,Z)\n",
    "    \n",
    "    assert(H.shape == Z.shape)\n",
    "    \n",
    "    relu_memory = Z \n",
    "    return H, relu_memory\n",
    "\n",
    "def softmax(Z):\n",
    "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
    "\n",
    "    Z_exp = np.exp(Z)\n",
    "\n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "    \n",
    "    return H, softmax_memory\n",
    "\n",
    "\n",
    "H, softmax_memory = softmax(Z)\n",
    "\n",
    "def initialize_parameters(dimensions):\n",
    "\n",
    "    # dimensions is a list containing the number of neuron in each layer in the network\n",
    "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "\n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    "\n",
    "    for l in range(1, L): \n",
    "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
    "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "dimensions  = [784, 3,7,10]\n",
    "parameters = initialize_parameters(dimensions)\n",
    "\n",
    "def layer_forward(H_prev, W, b, activation = 'sigmoid'):\n",
    "\n",
    "    # H_prev is of shape (size of previous layer, number of examples)\n",
    "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
    "    # b is bias vector of shape (size of the current layer, 1)\n",
    "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
    "\n",
    "    # H is the output of the activation function \n",
    "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z = np.dot(W, H_prev)+b\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = sigmoid(Z) #\n",
    " \n",
    "    elif activation == \"softmax\":\n",
    "        Z = np.dot(W, H_prev)+b#\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = softmax(Z) #\n",
    "    elif activation == \"relu\":\n",
    "        Z = np.dot(W, H_prev)+b#\n",
    "        linear_memory = (H_prev, W, b)\n",
    "        H, activation_memory = relu(Z) #\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "\n",
    "    return H, memory\n",
    "\n",
    "\n",
    "\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    # X is input data of shape (input size, number of examples)\n",
    "    # parameters is output of initialize_parameters()\n",
    "    \n",
    "    # HL is the last layer's post-activation value\n",
    "    # memories is the list of memory containing (for a relu activation, for example):\n",
    "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
    "    # - the memory of softmax forward (there is one, indexed L) \n",
    "\n",
    "    memories = []\n",
    "    H = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
    "    for l in range(1, L):\n",
    "        H_prev = H\n",
    "        W_l = parameters['W'+str(l)]\n",
    "        b_l = parameters['b'+str(l)]\n",
    "        H, memory = layer_forward(H_prev, W_l , b_l , activation = 'sigmoid')\n",
    "        memories.append(memory)\n",
    "    \n",
    "    # Implement the final softmax layer\n",
    "    # HL here is the final prediction P as specified in the lectures\n",
    "    W_L = parameters['W'+str(L)]\n",
    "    b_L = parameters['b'+str(L)]\n",
    "    HL, memory = layer_forward(H, W_L, b_L, activation = 'softmax') #\n",
    "    \n",
    "    memories.append(memory)\n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories\n",
    "\n",
    "def compute_loss(HL, Y):\n",
    "\n",
    "    # loss is the cross-entropy loss\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    loss = -(1.0/m) * np.sum(Y*np.log(HL)) \n",
    "    \n",
    "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a sigmoid function\n",
    "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory\n",
    "    \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dH, relu_memory):\n",
    "    \n",
    "    # Implement the backpropagation of a relu function\n",
    "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def layer_backward(dH, memory, activation = 'sigmoid'):\n",
    "    \n",
    "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "    # performs the backprop depending upon the activation function\n",
    "    \n",
    "\n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)#\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1.0/m) * np.matmul(dZ, H_prev.T)\n",
    "        db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "        dH_prev = np.matmul(np.transpose(W), dZ)#\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory)#\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1.0/m) * np.matmul(dZ, H_prev.T)\n",
    "        db = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "        dH_prev = np.matmul(np.transpose(W), dZ)#\n",
    "    \n",
    "    return dH_prev, dW, db\n",
    "\n",
    "def L_layer_backward(HL, Y, memories):\n",
    "    \n",
    "    # Takes the predicted value HL and the true target value Y and the \n",
    "    # memories calculated by L_layer_forward as input\n",
    "    \n",
    "    # returns the gradients calulated for all the layers as a dict\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    \n",
    "    \n",
    "    # Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    # Use the expressions you have used in 'layer_backward'\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(np.transpose(W),dZ)#\n",
    "    gradients[\"dW\" + str(L)] = (1.0/m) * np.matmul(dZ, H_prev.T)\n",
    "    gradients[\"db\" + str(L)] = (1.0/m) * np.sum(dZ, axis=-1, keepdims=True)\n",
    "\n",
    "    # Perform the backpropagation l-1 times\n",
    "    for l in reversed(range(L-1)):\n",
    "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\"+ str(l+1)] ,current_memory,activation=\"relu\")#\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp#\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp#\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp#\n",
    "\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "\n",
    "    # returns updated weights after applying the gradient descent update\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*gradients[\"dW\" + str(l + 1)] #\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*gradients[\"db\" + str(l + 1)]#\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "print(parameters)\n",
    "\n",
    "dimensions = [784, 10, 10]\n",
    "\n",
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    "\n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    "\n",
    "\n",
    "    # Forward propagation\n",
    "    HL, memories =  L_layer_forward(X, parameters)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = compute_loss(HL, Y)\n",
    "# Backward propagation\n",
    "    gradients = L_layer_backward(HL,Y, memories)\n",
    "\n",
    "    # Update parameters.\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "       \n",
    "\n",
    "L_layer_model(x.T, y.T, dimensions, learning_rate = 0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 103.20855646084341\n",
      "Epoch 10, Loss: 103.56587505530786\n",
      "Epoch 20, Loss: 103.9958522928392\n",
      "Epoch 30, Loss: 104.5877304440424\n",
      "Epoch 40, Loss: 104.32442370806034\n",
      "Epoch 50, Loss: 104.04254032917541\n",
      "Epoch 60, Loss: 103.79496847329922\n",
      "Epoch 70, Loss: 103.58644238351424\n",
      "Epoch 80, Loss: 103.41318448444656\n",
      "Epoch 90, Loss: 103.26969520727963\n"
     ]
    }
   ],
   "source": [
    "def initialize_momentum(parameters):\n",
    "    # Initialize momentum for each parameter with zeros\n",
    "    L = len(parameters) // 2\n",
    "    momentum = {}\n",
    "    for l in range(1, L + 1):\n",
    "        momentum[f'dW{l}'] = np.zeros_like(parameters[f'W{l}'])\n",
    "        momentum[f'db{l}'] = np.zeros_like(parameters[f'b{l}'])\n",
    "    return momentum\n",
    "\n",
    "def update_parameters_with_momentum(parameters, gradients, momentum, learning_rate, beta):\n",
    "    # Update parameters with momentum\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        momentum[f'dW{l}'] = beta * momentum[f'dW{l}'] + (1 - beta) * gradients[f'dW{l}']\n",
    "        momentum[f'db{l}'] = beta * momentum[f'db{l}'] + (1 - beta) * gradients[f'db{l}']\n",
    "\n",
    "        parameters[f'W{l}'] -= learning_rate * momentum[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * momentum[f'db{l}']\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def L_layer_model_with_momentum(X, Y, dimensions, learning_rate=0.0075, num_epochs=100, beta=0.9):\n",
    "    np.random.seed(2)\n",
    "    losses = []  # keep track of loss\n",
    "\n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    "    momentum = initialize_momentum(parameters)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories)\n",
    "\n",
    "        # Update parameters with momentum\n",
    "        parameters = update_parameters_with_momentum(parameters, gradients, momentum, learning_rate, beta)\n",
    "\n",
    "        # Print the loss every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "    return parameters, losses\n",
    "\n",
    "# Assuming x.T and y.T are your training data and labels\n",
    "trained_parameters_with_momentum, training_losses_with_momentum = L_layer_model_with_momentum(\n",
    "    x.T, y.T, dimensions, learning_rate=0.0075, num_epochs=100, beta=0.9\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
